Perfect! Hereâ€™s a \*\*ready-to-use `README.md`\*\* for your \*\*LLM-Evaluater\*\* repo:



````markdown

\# LLM-Evaluater



Evaluates LLM messages and responses stored in a Supabase Airtable using few-shot GPT prompts, then logs categorized results with severity levels into an evaluation table.



---



\## ğŸš€ Features



\- Connects to Airtable via pyairtable.  

\- Retrieves chat histories and evaluation records.  

\- Evaluates responses using GPT-4 few-shot prompts.  

\- Outputs structured JSON:  

&nbsp; - `category` â†’ Type of issue (None, Database, Persona, Knowledge Gap, Other)  

&nbsp; - `explanation` â†’ Short description of the issue  

&nbsp; - `severity` â†’ Scale 0 (no issue) to 5 (critical)  

\- Saves results back into an Airtable evaluation table.  

\- Supports graceful stopping (Ctrl+C) without errors.



---



\## âš™ï¸ Setup



1\. \*\*Clone the repository\*\*  

```bash

git clone https://github.com/heathrod/LLM-Evaluater.git

cd LLM-Evaluater

````



2\. \*\*Create a virtual environment (optional but recommended)\*\*



```bash

python -m venv venv

source venv/bin/activate  # Mac/Linux

venv\\Scripts\\activate     # Windows

```



3\. \*\*Install dependencies\*\*



```bash

pip install -r requirements.txt

```



4\. \*\*Create a `.env` file\*\* in the project root:



```env

AIRTABLE\_API\_KEY=your\_airtable\_api\_key

BASE\_ID\_1=your\_first\_base\_id

TABLE\_NAME\_1=Chat Histories

BASE\_ID\_2=your\_second\_base\_id 

TABLE\_NAME\_2=eval table

OPENAI\_API\_KEY=your\_openai\_api\_key

```



5\. \*\*Run the script\*\*



```bash

python main.py

```



---



\## ğŸ“‚ Project Structure



```

.

â”œâ”€â”€ main.py          # Main script

â”œâ”€â”€ .env             # Environment variables (ignored in Git)

â”œâ”€â”€ .gitignore       # Ignore secrets \& cache files

â”œâ”€â”€ requirements.txt # Python dependencies

â””â”€â”€ README.md        # Project description

```



---



\## âœ… Example Output



```json

{

&nbsp; "category": "Knowledge Gap",

&nbsp; "explanation": "The model didnâ€™t understand the user's domain-specific question.",

&nbsp; "severity": 3

}

```



```





